{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aromie/Python/blob/master/lesson17_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaCXdXETusAp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "imdb, info = tfds.load('imdb_reviews', with_info = True, as_supervised = True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(info)"
      ],
      "metadata": {
        "id": "UyMn8R-yu-vP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "train_data , test_data = imdb['train'] , imdb['test']\n",
        "\n",
        "\n",
        "for text, label in train_data:\n",
        "  print(text)\n",
        "  print(label)\n",
        "  break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygW_y9JlvHXR",
        "outputId": "e3746766-dec1-431a-a9cd-a76a7de7e300"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\", shape=(), dtype=string)\n",
            "tf.Tensor(0, shape=(), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_sentence = []\n",
        "training_labels = []\n",
        "\n",
        "testing_sentence = []\n",
        "testing_labels = []\n"
      ],
      "metadata": {
        "id": "SgnTAr67weYp"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence, label in train_data:\n",
        "  training_sentence.append(sentence.numpy().decode('utf8'))\n",
        "  training_labels.append(label.numpy())\n",
        "\n",
        "for sentence, label in test_data:\n",
        "  testing_sentence.append(sentence.numpy().decode('utf8'))\n",
        "  testing_labels.append(label.numpy())\n",
        "\n"
      ],
      "metadata": {
        "id": "q7CdVLAMxCOw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = np.array(training_labels)\n",
        "testing_labels = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "X4LxYyRExUy6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "vocab_size = 20000\n",
        "# oov :out of vocab\n",
        "tokenizer = Tokenizer(num_words = vocab_size , oov_token = '[oov]')\n",
        "tokenizer.fit_on_texts(training_sentence)\n",
        "word_dict = tokenizer.word_index"
      ],
      "metadata": {
        "id": "jjzeqZVWyi-b"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence = tokenizer.texts_to_sequences(training_sentence)\n",
        "sequence[:10]"
      ],
      "metadata": {
        "id": "CqpaYvBcz4qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# padding 길이 맞춰주기\n",
        "MAXLENGTH = 120\n",
        "sequence = pad_sequences(sequence, maxlen=MAXLENGTH)\n",
        "sequence[:10]"
      ],
      "metadata": {
        "id": "DwS6rH7f0LVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequence = tokenizer.texts_to_sequences(testing_sentence)\n",
        "test_sequence = pad_sequences(test_sequence, maxlen=MAXLENGTH)"
      ],
      "metadata": {
        "id": "U_6o00AYz-ep"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_dict = dict([(value, key) for key, value in word_dict.items()])"
      ],
      "metadata": {
        "id": "DR-Vrn-N0rR6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_dict"
      ],
      "metadata": {
        "id": "VzVLoYe02XEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_review(text):\n",
        "  result=[]\n",
        "  for idx in text:\n",
        "    word = idx_dict.get(idx, '*')\n",
        "    result.append(word)\n",
        "  return ' '.join(result)\n",
        "\n",
        "print('original :', training_sentence[0])\n",
        "print('seq', sequence[0])\n",
        "print('decoding : ', decode_review(sequence[0]))"
      ],
      "metadata": {
        "id": "rJ5R9CVo2urM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=20000\n",
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(vocab_size, 100, input_length=MAXLENGTH),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(6,activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')])\n",
        "\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "model.fit(sequence,training_labels, epochs=10, validation_data=(test_sequence, testing_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpQBa_ykCH3i",
        "outputId": "94014b4c-8532-4bf9-9285-1c298c2877bd"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 120, 100)          2000000   \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 12000)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 6)                 72006     \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 7         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,072,013\n",
            "Trainable params: 2,072,013\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "782/782 [==============================] - 30s 37ms/step - loss: 0.4131 - accuracy: 0.7982 - val_loss: 0.3310 - val_accuracy: 0.8556\n",
            "Epoch 2/10\n",
            "782/782 [==============================] - 29s 37ms/step - loss: 0.0872 - accuracy: 0.9734 - val_loss: 0.4091 - val_accuracy: 0.8340\n",
            "Epoch 3/10\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 0.0080 - accuracy: 0.9989 - val_loss: 0.4447 - val_accuracy: 0.8479\n",
            "Epoch 4/10\n",
            "782/782 [==============================] - 27s 34ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.4668 - val_accuracy: 0.8536\n",
            "Epoch 5/10\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 4.5621e-04 - accuracy: 1.0000 - val_loss: 0.4882 - val_accuracy: 0.8553\n",
            "Epoch 6/10\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 2.3992e-04 - accuracy: 1.0000 - val_loss: 0.5111 - val_accuracy: 0.8563\n",
            "Epoch 7/10\n",
            "782/782 [==============================] - 27s 35ms/step - loss: 1.3956e-04 - accuracy: 1.0000 - val_loss: 0.5322 - val_accuracy: 0.8563\n",
            "Epoch 8/10\n",
            "782/782 [==============================] - 27s 35ms/step - loss: 8.3980e-05 - accuracy: 1.0000 - val_loss: 0.5523 - val_accuracy: 0.8575\n",
            "Epoch 9/10\n",
            "782/782 [==============================] - 28s 36ms/step - loss: 5.1647e-05 - accuracy: 1.0000 - val_loss: 0.5726 - val_accuracy: 0.8574\n",
            "Epoch 10/10\n",
            "782/782 [==============================] - 30s 39ms/step - loss: 3.2229e-05 - accuracy: 1.0000 - val_loss: 0.5926 - val_accuracy: 0.8569\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb56cdd450>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for idx in range(len(model.layers)):\n",
        "  model.layers[idx]\n",
        "  "
      ],
      "metadata": {
        "id": "aoqcep-ADyjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layers=model.layers[0]\n",
        "weights = embedding_layers.get_weights()[0]\n",
        "print(weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBEHpQSED-PS",
        "outputId": "0ecb12fe-cc67-438b-9dac-a4ac496c5b1d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcKIVVJ0CH6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8n3cOvDKCH9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NlLa0o0FCIBA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}